# ScrapyHub

ScrapyHubæ˜¯ä¸€ä¸ªåŸºäºScrapy 2.14æ¡†æ¶çš„å¤šç½‘ç«™çƒ­æ¦œæ•°æ®é‡‡é›†ç³»ç»Ÿï¼Œç›®å‰æ”¯æŒå¾®åšã€ç™¾åº¦ã€æŠ–éŸ³ã€åå°”è¡—è§é—»ã€æ¾æ¹ƒç­‰çƒ­æœæ¦œå•çˆ¬å–ï¼Œé€šè¿‡å®šæ—¶é‡‡é›†å­˜å‚¨çƒ­æ¦œæ•°æ®ï¼Œå¯ç”¨äºæ•°æ®åˆ†æå’Œè¶‹åŠ¿ç ”ç©¶ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹ç‚¹

- **ç»Ÿä¸€é¡¹ç›®æ¶æ„**ï¼šé‡‡ç”¨å•ä¸€Scrapyé¡¹ç›®ï¼Œå¤šSpiderè®¾è®¡ï¼Œä¾¿äºç®¡ç†å’Œç»´æŠ¤
- **å¤šç«™ç‚¹æ•°æ®é‡‡é›†**ï¼šæ”¯æŒå¾®åšã€ç™¾åº¦ã€æŠ–éŸ³ã€åå°”è¡—è§é—»ã€æ¾æ¹ƒç­‰çƒ­æ¦œçˆ¬å–
- **æ™ºèƒ½æ•°æ®å»é‡**ï¼š3å¤©å†…ç›¸åŒæ¡ç›®è‡ªåŠ¨æ›´æ–°è€Œéé‡å¤æ’å…¥
- **è¿æ¥æ± ç®¡ç†**ï¼šä½¿ç”¨DBUtilså®ç°æ•°æ®åº“è¿æ¥æ± ï¼Œæå‡æ€§èƒ½
- **ç¯å¢ƒé…ç½®åˆ†ç¦»**ï¼šé€šè¿‡.envæ–‡ä»¶ç®¡ç†æ•æ„Ÿé…ç½®ï¼Œæé«˜å®‰å…¨æ€§
- **ç»Ÿä¸€Pipeline**ï¼šæ‰€æœ‰çˆ¬è™«å…±äº«æ•°æ®å¤„ç†é€»è¾‘ï¼Œå‡å°‘ä»£ç é‡å¤

## ğŸ“‹ é¡¹ç›®ç»“æ„

```
ScrapyHub/
â”œâ”€â”€ .env                      # ç¯å¢ƒå˜é‡é…ç½®(ä¸æäº¤åˆ°Git)
â”œâ”€â”€ .env.example              # ç¯å¢ƒå˜é‡é…ç½®æ¨¡æ¿
â”œâ”€â”€ .gitignore                # Gitå¿½ç•¥æ–‡ä»¶é…ç½®
â”œâ”€â”€ README.md                 # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ requirements.txt          # Pythonä¾èµ–åŒ…
â”œâ”€â”€ scrapy.cfg                # Scrapyé…ç½®æ–‡ä»¶
â”œâ”€â”€ run.py                    # å•ä¸ªçˆ¬è™«è¿è¡Œè„šæœ¬
â”œâ”€â”€ run_all.py                # æ‰¹é‡è¿è¡Œæ‰€æœ‰çˆ¬è™«
â”œâ”€â”€ logs/                     # æ—¥å¿—ç›®å½•
â”‚   â”œâ”€â”€ weibo.log
â”‚   â”œâ”€â”€ baidu.log
â”‚   â”œâ”€â”€ douyin.log
â”‚   â”œâ”€â”€ wallstreetcn.log
â”‚   â””â”€â”€ thepaper.log
â””â”€â”€ scrapyhub/                # ä¸»é¡¹ç›®åŒ…
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ settings.py           # ç»Ÿä¸€é…ç½®æ–‡ä»¶
    â”œâ”€â”€ items.py              # Itemå®šä¹‰
    â”œâ”€â”€ middlewares.py        # ä¸­é—´ä»¶
    â”œâ”€â”€ pipelines.py          # æ•°æ®ç®¡é“
    â”œâ”€â”€ spiders/               # çˆ¬è™«ç›®å½•
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ weibo_spider.py   # å¾®åšçƒ­æœ
    â”‚   â”œâ”€â”€ baidu_spider.py   # ç™¾åº¦çƒ­æ¦œ
    â”‚   â”œâ”€â”€ kr36_spider.py    # 36æ°ªçƒ­æ¦œ
    â”‚   â”œâ”€â”€ douyin_spider.py  # æŠ–éŸ³çƒ­æ¦œ
    â”‚   â”œâ”€â”€ wallstreetcn_spider.py  # åå°”è¡—è§é—»
    â”‚   â””â”€â”€ thepaper_spider.py      # æ¾æ¹ƒæ–°é—»
    â””â”€â”€ utils/                # å·¥å…·å‡½æ•°
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ db_utils.py       # æ•°æ®åº“è¿æ¥æ± ç®¡ç†
        â””â”€â”€ time_utils.py     # æ—¶é—´å¤„ç†å·¥å…·
```

## ğŸ› ï¸ ç¯å¢ƒé…ç½®

### ç³»ç»Ÿè¦æ±‚

- Python 3.13+
- MySQL 5.7+

### ä¾èµ–å®‰è£…

1. **åˆ›å»ºå¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ**

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv .venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate
```

2. **å®‰è£…é¡¹ç›®ä¾èµ–**

```bash
pip install -r requirements.txt
```

### æ•°æ®åº“é…ç½®

1. **åˆ›å»ºæ•°æ®åº“**

```sql
CREATE DATABASE hot_list CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
```

2. **é…ç½®ç¯å¢ƒå˜é‡**

å¤åˆ¶ [.env.example](.env.example) ä¸º `.env` å¹¶å¡«å…¥å®é™…é…ç½®ï¼š

```bash
# å¤åˆ¶é…ç½®æ–‡ä»¶
cp .env.example .env

# ç¼–è¾‘é…ç½®ï¼ˆå¡«å…¥å®é™…çš„æ•°æ®åº“ä¿¡æ¯ï¼‰
vim .env
```

`.env` æ–‡ä»¶å†…å®¹ç¤ºä¾‹ï¼š

```env
# MySQLæ•°æ®åº“é…ç½®
MYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_USER=root
MYSQL_PASSWORD=your_password
MYSQL_DATABASE=hot_list
MYSQL_CHARSET=utf8mb4

# æ—¥å¿—çº§åˆ«
LOG_LEVEL=INFO
```

## ğŸ•¸ï¸ çˆ¬è™«è¿è¡Œ

### è¿è¡Œå•ä¸ªçˆ¬è™«

ä½¿ç”¨ [run.py](run.py) è„šæœ¬è¿è¡ŒæŒ‡å®šçˆ¬è™«ï¼š

```bash
# èµ‹äºˆè„šæœ¬å¯æ‰§è¡Œæƒé™
chmod +x run.py
chmod +x run_all.py

# æŸ¥çœ‹å¸®åŠ©
python run.py

# è¿è¡Œå¾®åšçƒ­æœçˆ¬è™«
python run.py weibo

# è¿è¡Œç™¾åº¦çƒ­æ¦œçˆ¬è™«
python run.py baidu

# è¿è¡ŒæŠ–éŸ³çƒ­æ¦œçˆ¬è™«
python run.py douyin

# è¿è¡Œåå°”è¡—è§é—»çˆ¬è™«
python run.py wallstreetcn

# è¿è¡Œæ¾æ¹ƒæ–°é—»çˆ¬è™«
python run.py thepaper
```

### æ‰¹é‡è¿è¡Œæ‰€æœ‰çˆ¬è™«

ä½¿ç”¨ [run_all.py](run_all.py) ä¸€æ¬¡æ€§è¿è¡Œæ‰€æœ‰çˆ¬è™«ï¼š

```bash
python run_all.py
```

### ä½¿ç”¨Scrapyå‘½ä»¤

ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨Scrapyå‘½ä»¤è¡Œï¼š

```bash
# åˆ—å‡ºæ‰€æœ‰çˆ¬è™«
scrapy list

# è¿è¡ŒæŒ‡å®šçˆ¬è™«
scrapy crawl weibo
```

## â° å®šæ—¶ä»»åŠ¡é…ç½®

### Linux/macOS (Crontab)

```bash
# ç¼–è¾‘crontab
crontab -e

# æ¯10åˆ†é’Ÿè¿è¡Œä¸€æ¬¡å¾®åšçˆ¬è™«
*/10 * * * * cd /opt/ScrapyHub && /opt/ScrapyHub/.venv/bin/python /opt/ScrapyHub/run.py weibo >> logs/weibo.log 2>&1

# æ¯å°æ—¶è¿è¡Œä¸€æ¬¡æ‰€æœ‰çˆ¬è™«
0 * * * * cd /opt/ScrapyHub && /opt/ScrapyHub/.venv/bin/python /opt/ScrapyHub/run_all.py >> /opt/ScrapyHub/logs/run_all.log 2>&1

# é‡å¯cronæœåŠ¡
sudo service cron restart
```

### æ—¥å¿—è½®è½¬é…ç½®

é˜²æ­¢æ—¥å¿—æ–‡ä»¶è¿‡å¤§ï¼š

```bash
# å®‰è£…logrotate
sudo apt install logrotate -y

# åˆ›å»ºé…ç½®æ–‡ä»¶
sudo vim /etc/logrotate.d/scrapyhub
```

é…ç½®å†…å®¹ï¼š

```
/path/to/ScrapyHub/logs/*.log {
    daily
    missingok
    rotate 7
    compress
    delaycompress
    notifempty
    create 0640 user user
}
```

## ğŸ“Š æ•°æ®ç»“æ„

æ•°æ®åº“è¡¨ `ranking` ç»“æ„ï¼š

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| id | INT | è‡ªå¢ä¸»é”® |
| title | VARCHAR(255) | çƒ­æœæ ‡é¢˜ |
| url | VARCHAR(512) | çƒ­æœé“¾æ¥ |
| hot_rank | INT | æ’å |
| source | TINYINT | æ¥æºï¼ˆ1-å¾®åšï¼Œ2-çŸ¥ä¹ï¼Œ3-ç™¾åº¦ï¼Œ4-36æ°ªï¼Œ5-æŠ–éŸ³ï¼Œ6-åå°”è¡—è§é—»ï¼Œ7-æ¾æ¹ƒï¼‰ |
| batch_timestamp | TIMESTAMP | æ‰¹æ¬¡æ—¶é—´æˆ³ |
| created_at | TIMESTAMP | è®°å½•åˆ›å»ºæ—¶é—´ |

ç´¢å¼•ï¼š
- `idx_batch_timestamp`: æ‰¹æ¬¡æ—¶é—´ç´¢å¼•
- `idx_title`: æ ‡é¢˜ç´¢å¼•
- `idx_source`: æ¥æºç´¢å¼•

## ğŸ”§ æŠ€æœ¯æ¶æ„

### æ ¸å¿ƒç»„ä»¶

1. **Items** ([scrapyhub/items.py](scrapyhub/items.py))
   - `RankingItem`: ç»Ÿä¸€çš„æ’è¡Œæ¦œæ•°æ®é¡¹

2. **Pipelines** ([scrapyhub/pipelines.py](scrapyhub/pipelines.py))
   - `RankingPipeline`: ç»Ÿä¸€çš„æ•°æ®å¤„ç†ç®¡é“ï¼Œæ”¯æŒå»é‡å’Œæ‰¹æ¬¡ç®¡ç†

3. **Middlewares** ([scrapyhub/middlewares.py](scrapyhub/middlewares.py))
   - `RandomUserAgentMiddleware`: éšæœºUser-Agentä¸­é—´ä»¶

4. **Utils** ([scrapyhub/utils/](scrapyhub/utils/))
   - [`db_utils.py`](scrapyhub/utils/db_utils.py): æ•°æ®åº“è¿æ¥æ± ç®¡ç†
   - [`time_utils.py`](scrapyhub/utils/time_utils.py): æ—¶é—´å¤„ç†å·¥å…·

### è®¾è®¡ç‰¹ç‚¹

- **å•ä¸€èŒè´£**ï¼šæ¯ä¸ªSpideråªè´Ÿè´£æ•°æ®é‡‡é›†ï¼Œæ•°æ®å¤„ç†ç»Ÿä¸€äº¤ç»™Pipeline
- **é…ç½®åˆ†ç¦»**ï¼šæ•æ„Ÿé…ç½®é€šè¿‡ç¯å¢ƒå˜é‡ç®¡ç†
- **è¿æ¥æ± **ï¼šä½¿ç”¨DBUtilsè¿æ¥æ± ï¼Œæå‡æ•°æ®åº“æ“ä½œæ€§èƒ½
- **æ‰¹æ¬¡ç®¡ç†**ï¼šæ¯æ¬¡è¿è¡Œä½¿ç”¨ç»Ÿä¸€çš„batch_timestampæ ‡è¯†æ‰¹æ¬¡
- **è‡ªåŠ¨å»é‡**ï¼š3å¤©å†…ç›¸åŒæ¥æº+æ ‡é¢˜çš„æ•°æ®è‡ªåŠ¨æ›´æ–°è€Œéé‡å¤æ’å…¥

## ğŸ› å¸¸è§é—®é¢˜

### 1. æ•°æ®åº“è¿æ¥å¤±è´¥

**é—®é¢˜**ï¼šè¿è¡Œçˆ¬è™«æ—¶æŠ¥æ•°æ®åº“è¿æ¥é”™è¯¯

**è§£å†³**ï¼š
- æ£€æŸ¥ [.env](.env) ä¸­çš„æ•°æ®åº“é…ç½®æ˜¯å¦æ­£ç¡®
- ç¡®è®¤MySQLæœåŠ¡æ­£åœ¨è¿è¡Œ
- ç¡®è®¤æ•°æ®åº“å·²åˆ›å»ºä¸”ç”¨æˆ·æœ‰æƒé™

### 2. çˆ¬è™«æ— æ³•è·å–æ•°æ®

**é—®é¢˜**ï¼šçˆ¬è™«è¿è¡Œä½†æ²¡æœ‰æ•°æ®å…¥åº“

**è§£å†³**ï¼š
- ç›®æ ‡ç½‘ç«™å¯èƒ½æ›´æ–°äº†APIæˆ–åçˆ¬ç­–ç•¥
- æŸ¥çœ‹å¯¹åº”çš„æ—¥å¿—æ–‡ä»¶ï¼ˆå¦‚ `logs/weibo.log`ï¼‰
- æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
- è€ƒè™‘å¢åŠ ä¸‹è½½å»¶è¿Ÿ ([scrapyhub/settings.py](scrapyhub/settings.py) ä¸­çš„ `DOWNLOAD_DELAY`)

### 3. æ—¥å¿—æ–‡ä»¶è¿‡å¤§

**é—®é¢˜**ï¼šlogsç›®å½•å ç”¨ç©ºé—´è¿‡å¤§

**è§£å†³**ï¼š
- é…ç½®logrotateè¿›è¡Œæ—¥å¿—è½®è½¬
- truncate -s 0 /path/to/your.log å°†æ–‡ä»¶æˆªæ–­ä¸º0å­—èŠ‚

### 4. 36æ°ªçˆ¬è™«è·å–ä¸åˆ°æ•°æ®

**é—®é¢˜**ï¼š36æ°ªæ¦œå•é¡µé¢ç»“æ„å˜åŒ–

**è§£å†³**ï¼š
- 36æ°ªçš„é€‰æ‹©å™¨å¯èƒ½éœ€è¦æ›´æ–°
- æŸ¥çœ‹ [scrapyhub/spider/kr36_spider.py](scrapyhub/spider/kr36_spider.py) ä¸­çš„é€‰æ‹©å™¨é…ç½®
- å°è¯•è®¿é—®ç›®æ ‡URLæ£€æŸ¥é¡µé¢ç»“æ„

## ğŸ“ å¼€å‘æŒ‡å—

### æ·»åŠ æ–°çˆ¬è™«

1. **åˆ›å»ºSpideræ–‡ä»¶**

```
# scrapyhub/spider/example_spider.py
import scrapy
from ..items import RankingItem

class ExampleSpider(scrapy.Spider):
    name = 'example'
    allowed_domains = ['example.com']
    start_urls = ['https://example.com/hot']
    
    custom_settings = {
        'LOG_FILE': 'logs/example.log',
    }
    
    def parse(self, response):
        # è§£æé€»è¾‘
        for item in response.css('.hot-item'):
            yield RankingItem(
                title=item.css('.title::text').get(),
                url=item.css('a::attr(href)').get(),
                hot_rank=item.css('.rank::text').get(),
                source=self.name
            )
```